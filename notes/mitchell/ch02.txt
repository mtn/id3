# ch02

Central problem: inducing general functions from specific training examples

Concept learning: inferring a boolean-valued function from training examples

Here, we consider a conjunction of constraints and try to figure out which correspond with the outcomes
    ex. Aldo enjoys water skiing on all days where it's Cold and Humid

Inductive assumption: we can at best guarantee that our classifier is correct for training data.
We apply an inductive assumption that our training data applies test data when applying it

One way of conceptualizing concept learning is as a search through the hypothesis space for the hypothesis which best fits the training examples
    - Thus we think about algorithms to efficiently search the hypothesis space

    General-to-specific hypothesis ordering:
        h1 = (Sunny, ?, ?, Strong, ?, ?)
        h2 = (Sunny,?, ?, ?, ?, ?)

        Here, h2 classifies a superset of h1, so we call it more general.
        More rigorously, any instance that satisfies h1 also satisfies h2.

    Find-S Algorithm:
        - We start with the most specific possible hypothesis and make it incrementally more general it with each training example
        - If we hypothesize that there exists a true target concept c, then we know that we never have to update in response to negative examples and can thus ignore them
            - At a given moment, our classifier is as specific as it can possibly be, and would not produce false positives on this assumption
        Problem: noise in practical training data can severely throw off the result

    Candidate Elimination Algorithm:
        Key idea: output all hypotheses that are consistent with the training data
        However, also performs poorly on noisy training data

Def: We say a given hypothesis h is consistent with the concept c if h(x) == c(x) for all x in the training set
Def: We say a given example x satisfies a hypothesis h if h(x) == 1 (regardless of accuracy)
Def: The version space wrt a hypothesis space is the set of all hypotheses that are consistent with training examples

    List-Then-Eliminate Algorithm:
        - List out entire hypothesis space, exhaustively eliminate those that doesn't satisfy the training example
        - Only realistic for trivial cases

        Candidate elimination works on the same principle, but produces a much more compact representation of the resulting version space
            - Represented by most and least general members
            Algorithm: Initialize G boundary to general hypothesis <?,?,?<?,...> and S boundary to most specific </,/,/,/,...>
                For each positive example, remove inconsistent hypotheses from G
                                           add to S minimal generalizations of h s.t. h remains consistent with d
                                           remove any "more-general" hypotheses from s
                    each negative example, remove any inconsistent hypothesis from s
                                            add to g the minimal specification s.t h is consistent with d and some s is more specific than h
                                            remove "less general" hypotheses from g
                    Negative examples force the general hypothesis to become more specific, and vice versa
                    Because the result contains all consistent hypotheses, order doesn't matter
                    The algo will converge given:
                        1) no errors in training data
                        2) there exists a hypothesis that describes the target concept

Query Strategies: what if the learner gets to request examples?
    - Should ask questions that shrink the hypothesis space. Ideally we can split the space with each question

Inductive Bias:
    By requiring conjunctive hypotheses, we can end up no hypotheses that are consistent with the examples.
    Unbiased hypotheses spaces - no basis for discriminating between hypotheses -> a learner must make assumptions to be able to make classifications
    We use this concept to describe our classification algorithms (ex. candidate elimination assumes the target concept exists in the hypothesis space)



